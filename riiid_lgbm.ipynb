{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Part I: Feature Engineering"},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nfrom numpy import savez_compressed\nimport  pandas as pd\npd.set_option('display.max_columns', 50)\nimport os\nos.makedirs('./compressed_features')\nimport gc","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Helper functions"},{"metadata":{"trusted":true},"cell_type":"code","source":"def np_compress(feature):\n    data = np.ndarray(shape=(train_df.shape[0], 1))\n    data = train_df[feature].values.astype(np_dict[feature])\n    # train_df.drop(columns=[feature], inplace=True)\n    print(f'The length of {feature}: {len(data)}')\n    print(data)\n    np.savez_compressed(f'./compressed_features/{feature}', data = data)\n\n# used for setting the index of a new DataFrame\nN_ROWS = 99271300\ndef get_index_np():\n    return np.arange(N_ROWS)\n\n# This function returns a feature\n# A list of indices and a data type can be passed to retrieve spcific training/validation rows as float32\ndef load_feature(feature, idxs=None, dtype=None):\n    file_path = f'/kaggle/input/riiid-answer-correctness-prediction-features-temp/FEATURES_V1G/{feature}.npz'\n    if idxs is None and dtype is None:\n        return np.load(file_path, allow_pickle=True)['v']\n    elif idxs is not None and dtype is not None:\n        return np.load(file_path, allow_pickle=True)['v'][idxs].astype(dtype)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"target = 'answered_correctly'\n\ndata_types_dict = {\n    'timestamp': 'int64',\n    'user_id': 'int32', \n    'content_id': 'int16', \n    'task_container_id': 'int16',\n    'answered_correctly': 'int8', \n    'prior_question_elapsed_time': 'float32', \n}\n                   \nfeatures_dtypes = {\n    'mean_user_accuracy': 'float32',\n    'answered_user': 'uint16',\n    'mean_content_accuracy': 'float32',\n    'content_count': 'int32',\n    'user_attempts': 'uint16',\n    'user_rating': 'float32',\n    'mean_user_part_accuracy': 'float32',\n    'part_cumcount': 'uint16',\n    'last_interaction_elapsed_time_l1': 'float64',\n    'last_interaction_elapsed_time_l2': 'float64',\n    'last_interaction_elapsed_time_l3': 'float64',\n    'prior_tag': 'int16',\n}\n\nnp_dict = {\n    'timestamp':np.float32,\n    'user_id':np.int32,\n    'content_id':np.int16,\n    'task_container_id':np.int16,\n    'answered_correctly':np.int8,\n    'mean_user_accuracy':np.float32,\n    'answered_user':np.int16,\n    'answered_correctly_user':np.int16,\n    'user_attempts':np.int16,\n    'hmean_user_content_accuracy':np.float32,\n    'mean_content_accuracy':np.float32,\n    'content_count':np.float32,\n    'user_rating':np.float32,\n    'part':np.int8,\n    'tags1':np.int8,\n    'tags2':np.int8,\n    'mean_user_part_accuracy':np.float32,\n    'last_interaction_elapsed_time_l1':np.float32,\n    'last_interaction_elapsed_time_l2':np.float32,\n    'last_interaction_elapsed_time_l3':np.float32,\n    'last_correct_time_elapsed':np.float32,\n    'last_incorrect_time_elapsed':np.float32,\n    'prior_question_elapsed_time':np.float32,\n}","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Load data"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df = pd.read_csv('/kaggle/input/riiid-test-answer-prediction/train.csv',\n                       low_memory=False, nrows=1_000_000,\n                       usecols=['timestamp',\n                                'user_id', \n                                'content_id', \n                                'task_container_id',\n                                'answered_correctly', \n                                'prior_question_elapsed_time', \n                                ],\n                       dtype=data_types_dict\n                       )\n\ntrain_df = train_df[train_df[target] != -1].reset_index(drop=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.prior_question_elapsed_time.fillna(23916, inplace = True)\nnp_compress('prior_question_elapsed_time')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* **last_interaction_elapsed_time_l1**\n* **last_interaction_elapsed_time_l2**\n* **last_interaction_elapsed_time_l2**"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df['timestamp'] = train_df['timestamp']/(1000*3600)\ntrain_df.timestamp = train_df.timestamp.astype('float32')\n\ntimestamp_df= train_df.groupby(['user_id', 'task_container_id']).head(1)[['user_id', 'task_container_id', 'timestamp']]\n\ntimestamp_df['last_interaction_elapsed_time_l1'] = timestamp_df.groupby('user_id')['timestamp'].shift()\ntimestamp_df['last_interaction_elapsed_time_l2'] = timestamp_df.groupby('user_id')['timestamp'].shift(2)\ntimestamp_df['last_interaction_elapsed_time_l3'] = timestamp_df.groupby('user_id')['timestamp'].shift(3)\n\ntimestamp_df.drop(columns=['timestamp'], inplace=True)\ntrain_df = pd.merge(train_df, timestamp_df, on=['user_id', 'task_container_id'], how='left')\n\ntime_diff1_mean = train_df['last_interaction_elapsed_time_l1'].mean()\ntime_diff2_mean = train_df['last_interaction_elapsed_time_l2'].mean()\ntime_diff3_mean = train_df['last_interaction_elapsed_time_l3'].mean()\n\ntrain_df.timestamp.fillna(0, inplace = True)\ntrain_df.last_interaction_elapsed_time_l1.fillna(time_diff1_mean, inplace = True)\ntrain_df.last_interaction_elapsed_time_l2.fillna(time_diff2_mean, inplace = True)\ntrain_df.last_interaction_elapsed_time_l3.fillna(time_diff3_mean, inplace = True)\n\n# del timestamp_df\n\nnp_compress('last_interaction_elapsed_time_l1')\nnp_compress('last_interaction_elapsed_time_l2')\nnp_compress('last_interaction_elapsed_time_l3')\nnp_compress('timestamp')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* **user_attempts**"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df[\"user_attempts\"] = 1\ntrain_df[\"user_attempts\"] = train_df[[\"user_id\",\"content_id\",\"user_attempts\"]].groupby([\"user_id\",\"content_id\"])[\"user_attempts\"].cumsum()\ntrain_df[\"user_attempts\"] = train_df[\"user_attempts\"].mask((train_df['user_attempts'] > 5), 5)\ntrain_df.user_attempts = train_df.user_attempts - 1\nprint(train_df['user_attempts'].value_counts())\nnp_compress('user_attempts')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* **mean_user_accuracy**\n* **answered_correctly_user**\n* **answered_user**"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df['lag'] = train_df[['user_id', 'answered_correctly']].groupby('user_id')[target].shift()\ncum = train_df[['user_id', 'lag']].groupby('user_id')['lag'].agg(['cumsum', 'cumcount'])\ntrain_df['mean_user_accuracy'] = cum['cumsum'] / cum['cumcount']\ntrain_df.mean_user_accuracy.fillna(0.680, inplace = True)\ncum.columns = ['answered_correctly_user', 'answered_user']\ntrain_df.drop(columns=['lag'], inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* **tags1**\n* **tags2**\n* **mean_content_accuracy**\n* **content_count**"},{"metadata":{"trusted":true},"cell_type":"code","source":"questions_df = pd.read_csv('../input/riiid-test-answer-prediction/questions.csv', \n                            usecols=['question_id', 'part', 'tags'],\n                            dtype={'question_id': 'int16', 'part': 'int8', 'tags': 'str'}\n                            )\n                          \ntag = questions_df[\"tags\"].str.split(\" \", n = 10, expand = True) \ntag.columns = ['tags1','tags2','tags3','tags4','tags5','tags6']\ntag.drop(columns=['tags3','tags4','tags5','tags6'], inplace=True)\n\nquestions_df =  pd.concat([questions_df,tag],axis=1)\nquestions_df.drop(columns=['tags'], inplace=True)\nquestions_df['tags1'] = pd.to_numeric(questions_df['tags1'], errors='coerce')\nquestions_df['tags2'] = pd.to_numeric(questions_df['tags2'], errors='coerce')\nquestions_df['tags2'].fillna(-1, inplace = True)\ndel tag\n\nquestions_df[['mean_content_accuracy', 'content_count']] = train_df[['content_id', 'answered_correctly']].groupby('content_id')[target].agg(['mean', 'count'])\ntrain_df = pd.merge(train_df, questions_df, left_on='content_id', right_on='question_id', how='left')\ntrain_df.drop(columns=['question_id'], inplace=True)\nnp_compress('tags1')\nnp_compress('tags2')\nnp_compress('content_count')\ndel(questions_df)\n\ntrain_df = train_df.join(cum)\ntrain_df.answered_correctly_user.fillna(0, inplace = True)\ntrain_df.answered_user.fillna(0.0, inplace = True)\nnp_compress('answered_correctly_user')\ndel(cum)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* **part**\n* **mean_user_part_accuracy**"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df['lag'] = train_df.groupby(['user_id', 'part'])[target].shift()\ncum = train_df.groupby(['user_id', 'part'])['lag'].agg(['cumsum', 'cumcount'])\ntrain_df['cumcount_p'] = cum['cumcount']\ntrain_df['mean_user_part_accuracy'] = cum['cumsum'] / cum['cumcount']\ntrain_df.drop(columns=['lag'], inplace=True)\n\nnp_compress('part')\nnp_compress('mean_user_part_accuracy')\ndel cum","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* **user_rating**"},{"metadata":{"trusted":true},"cell_type":"code","source":"cols = ['user_id', 'task_container_id', 'answered_user', 'mean_content_accuracy', 'answered_correctly']\n\ntrain_df['user_rating'] = train_df['answered_correctly'] - train_df['mean_content_accuracy']\ntrain_df['user_rating'] = train_df.groupby('user_id')['user_rating'].shift()\ntrain_df['user_rating'] = train_df.groupby('user_id')['user_rating'].cumsum()\n\ndf_ = train_df.groupby(['user_id', 'task_container_id']).head(1)[['user_id', 'task_container_id', 'user_rating']]\ntrain_df.drop(columns=['user_rating'], inplace=True)\ntrain_df = pd.merge(train_df, df_, on=['user_id', 'task_container_id'], how='left')\n\ntrain_df['user_rating'] = train_df['user_rating'] / train_df['answered_user']\ntrain_df['user_rating'].fillna(0, inplace=True)\n\nnp_compress('user_rating')\nnp_compress('task_container_id')\nnp_compress('answered_user')\ndel df_","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* **hmean_user_content_accuracy**"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df['hmean_user_content_accuracy'] = 2 * ((train_df['mean_user_accuracy'] *  train_df['mean_content_accuracy']) / (train_df['mean_user_accuracy'] + train_df['mean_content_accuracy']))\ntrain_df.hmean_user_content_accuracy.fillna(0.0, inplace = True)\n\nnp_compress('hmean_user_content_accuracy')\nnp_compress('mean_content_accuracy')\nnp_compress('mean_user_accuracy')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"np_compress('user_id')\nnp_compress('content_id')\nnp_compress('answered_correctly')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"n_rows = len(train_df)\nn_rows","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# del(train_df)\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Part II: Training"},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"import lightgbm as lgb\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom tqdm.notebook import tqdm\nfrom cairosvg import svg2png\nfrom PIL import Image\nfrom io import BytesIO\n\nfrom glob import glob\nimport pickle\nimport re","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# This simply shows all features\nfor file_path in glob('./compressed_features/*.npz'):\n    print(re.findall('(?<=.\\/)([a-z_0-9]*)(.npz)', file_path)[0][0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"# this are given features, bundle_id is retrieved by merging the questions df with the train df\ngiven_features = [\n    'prior_question_elapsed_time',\n    'prior_question_had_explanation',\n    # 'bundle_id',\n]\n\ndeduced_features = [\n    # user features\n    'mean_user_accuracy',\n    'answered_correctly_user',\n    'answered_user',\n    # content features\n    'mean_content_accuracy',\n    'content_count',\n    # part features\n    'part',\n    'mean_user_part_accuracy',\n    # tag features\n    'tag_1',\n    'tag_2',\n    # user content features\n    'hmean_user_content_accuracy',\n    # last interaction elapsed time\n    'last_interaction_elapsed_time_l1',\n    'last_interaction_elapsed_time_l2',\n    'last_interaction_elapsed_time_l3',\n    # other\n    'attempt',\n    # 'user_ratings',\n    \n    # lastly added features\n    'last_correct_time_elapsed',\n    'last_incorrect_time_elapsed',\n]\n\nfeatures = given_features + deduced_features\n\nfeatures_df_cols = [\n    'user_id', 'content_id', 'part', 'tags', # merge keys\n    'tags_label', 'answered_user', # deduced data\n    'answered_correctly_user', 'mean_user_accuracy', 'mean_content_accuracy', # deduced features\n]\n\ntarget = 'answered_correctly'\n\n# specify the indices of the columns with categorical features to LightGBM\ncategorical_feature = ['part', 'retry', 'prior_question_had_explanation', 'bundle_id', 'tag_1', 'tag_2']\ncategorical_feature_idxs = []\nfor v in categorical_feature:\n    try:\n        categorical_feature_idxs.append(features.index(v))\n    except:\n        pass","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Make train and validation datasets"},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"def get_train_val_idxs(TRAIN_SIZE, VAL_SIZE):\n    train_idxs = []\n    val_idxs = []\n    NEW_USER_FRAC = 1/4 # fraction of new users, 25% of validation rows are new users\n    np.random.seed(42)\n    \n    # create df with user_ids and indices\n    df = pd.DataFrame(index=get_index_np())\n    for feature in ['user_id']:\n        df[feature] = load_feature(feature)\n\n    df['index'] = df.index.values.astype(np.uint32)\n    user_id_index = df.groupby('user_id')['index'].apply(np.array)\n    \n    # iterate over users in random order\n    for indices in user_id_index.sample(user_id_index.size, random_state=42):\n        if len(train_idxs) > TRAIN_SIZE:\n            break\n\n        # fill validation data\n        if len(val_idxs) < VAL_SIZE:\n            # add new user\n            if np.random.rand() < NEW_USER_FRAC:\n                val_idxs += list(indices)\n            # randomly split user between train and val\n            else:\n                offset = np.random.randint(0, indices.size)\n                train_idxs += list(indices[:offset])\n                val_idxs += list(indices[offset:])\n        else:\n            train_idxs += list(indices)\n    \n    train_idxs = np.array(train_idxs, dtype=np.uint32)\n    val_idxs = np.array(val_idxs, dtype=np.uint32)\n    \n    return train_idxs, val_idxs\n\ntrain_idxs, val_idxs = get_train_val_idxs(int(1e6), 0.25e6)\nprint(f'len train_idxs: {len(train_idxs)}, len validation_idxs: {len(val_idxs)}')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"def make_x_y(train_idxs, val_idxs):\n    # create numpy arrays\n    X_train = np.ndarray(shape=(len(train_idxs), len(features)), dtype=np.float32)\n    X_val = np.ndarray(shape=(len(val_idxs), len(features)), dtype=np.float32)\n    \n    # now fill them up column wise to reduce memory usage\n    # features are loaded from disk as npz files (compressed numpy arrays)\n    for idx, feature in enumerate(tqdm(features)):\n        X_train[:,idx] = load_feature(feature, train_idxs, np.float32)\n        X_val[:,idx] = load_feature(feature, val_idxs, np.float32)\n    \n    y_train = load_feature(target, train_idxs, np.int8)\n    y_val = load_feature(target, val_idxs, np.int8)\n                         \n    return X_train, y_train, X_val, y_val\n    \nX_train, y_train, X_val, y_val = make_x_y(train_idxs, val_idxs)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"print(f'X_train.shape: {X_train.shape}\\t y_train.shape: {y_train.shape}')\nprint(f'X_val.shape: {X_val.shape}\\t y_val.shape: {y_val.shape}')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"# show train features\ndisplay(pd.DataFrame(X_train[:25], columns=features))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"# Check the target (answered correctly) as sanity check\ndisplay(y_train[:25])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"# make train and validation dataset\ntrain_data = lgb.Dataset(\n    data = X_train,\n    label = y_train,\n    categorical_feature = None,\n)\n\nval_data = lgb.Dataset(\n    data = X_val,\n    label = y_val,\n    categorical_feature = None,\n)\n\n# Free up RAM\ndel X_train, y_train, X_val, y_val, train_idxs, val_idxs\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Training"},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n\n# Simple LightGBM parameters\nlgbm_params = {\n    'objective': 'binary',\n    'metric': ['auc'],\n    'num_leaves': 200,\n    'learning_rate': 0.1,\n}\n\ndef train():\n    evals_result = {}\n    model = lgb.train(\n        params = lgbm_params,\n        train_set = train_data,\n        valid_sets = [val_data],\n        num_boost_round = 5000,\n        verbose_eval = 10,\n        evals_result = evals_result,\n        early_stopping_rounds = 10,\n        categorical_feature = categorical_feature_idxs,\n        feature_name = features,\n    ) \n\n    # save model\n    model.save_model(f'model.lgb')\n    \n    return model, evals_result\n    \nmodel, evals_result = train()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Training History"},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"def plot_history(evals_result):\n    for metric in ['auc']:\n        plt.figure(figsize=(20,8))\n        \n        for key in evals_result.keys():\n            history_len = len(evals_result.get(key)[metric])\n            history = evals_result.get(key)[metric]\n            x_axis = np.arange(1, history_len + 1)\n            plt.plot(x_axis, history, label=key)\n        \n        x_ticks = list(filter(lambda e: (e % (history_len // 100 * 10) == 0) or e == 1, x_axis))\n        plt.xticks(x_ticks, fontsize=12)\n        plt.yticks(fontsize=12)\n\n        plt.title(f'{metric.upper()} History of training', fontsize=18);\n        plt.xlabel('EPOCH', fontsize=16)\n        plt.ylabel(metric.upper(), fontsize=16)\n        \n        if metric in ['auc']:\n            plt.legend(loc='upper left', fontsize=14)\n        else:\n            plt.legend(loc='upper right', fontsize=14)\n        plt.grid()\n        plt.show()\n\nplot_history(evals_result)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"def show_feature_importances(model, importance_type, max_num_features=10**10):\n    feature_importances = pd.DataFrame()\n    feature_importances['feature'] = features\n    feature_importances['value'] = pd.DataFrame(model.feature_importance(importance_type))\n    feature_importances = feature_importances.sort_values(by='value', ascending=False) # sort feature importance\n    feature_importances.to_csv(f'feature_importances_{importance_type}.csv') # write feature importance to csv\n    feature_importances = feature_importances[:max_num_features] # only show max_num_features\n    \n    plt.figure(figsize=(18, 8))\n    plt.xlim([0, feature_importances.value.max()*1.1])\n    plt.title(f'Feature {importance_type}', fontsize=18);\n    sns.barplot(data=feature_importances, x='value', y='feature', palette='rocket');\n    for idx, v in enumerate(feature_importances.value):\n        plt.text(v, idx, \"  {:.2e}\".format(v))\n\nshow_feature_importances(model, 'gain')\nshow_feature_importances(model, 'split')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"# show tree and save as png\ndef save_tree_diagraph(model):\n    tree_digraph = lgb.create_tree_digraph(model, show_info=['split_gain', 'internal_count'])\n\n    tree_png = svg2png(tree_digraph._repr_svg_(), output_width=3840)\n    tree_png = Image.open(BytesIO(tree_png))\n\n    tree_png.save('create_tree_digraph.png')\n\n    display(tree_png)\n    \nsave_tree_diagraph(model)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"# remove train and validation data to free memory before prediction phase\ndel train_data, val_data\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Part III: Prediction"},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"def get_features_questions_df():\n    # create DataFrame of features\n    features_questions_df = pd.DataFrame(index=get_index_np())\n    cols = [\n        'content_id',\n        'part',\n        'tag_1',\n        'tag_2',\n        'content_count',\n        'bundle_id',\n    ]\n    \n    for feature in tqdm(cols):\n        features_questions_df[feature] = load_feature(feature)\n\n    # content features\n    features_questions_df.drop_duplicates(subset='content_id', inplace=True)\n    features_questions_df.sort_values('content_id', inplace=True)\n    features_questions_df.reset_index(drop=True, inplace=True)\n    \n    return features_questions_df\n    \nfeatures_questions_df = get_features_questions_df()\nprint(f'features_questions_df, rows: {features_questions_df.shape[0]}')\ndisplay(features_questions_df.head())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## STATE\n\nThis next function is the beating heart of my prediction phase, a massive dictionary to keep track of all features of all users and update them with every interaction.\n\nI agree, the code is somewhat unreadable, but the basic idea is as follows:\n\nCompute features over all user data (mean_user_accuracy, answered_user, answered_correctly_user) \nas these features have a lag of 1\n\nGet the last data point for other features (lecturs seen, mean_content_accuracy, etc)\n\nCreate a dictionary where for each user all features are kept track of, an example of a user is shown below the function"},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"def get_state():\n    # create DataFrame of features\n    features_df = pd.DataFrame(index=get_index_np())\n    \n    cols = ['user_id', 'content_id', 'answered_correctly', 'mean_content_accuracy', 'last_correct_time_elapsed', 'last_incorrect_time_elapsed',\n            'timestamp', 'last_interaction_elapsed_time_l1', 'last_interaction_elapsed_time_l2', 'last_interaction_elapsed_time_l3']\n    for f in tqdm(cols):\n        features_df[f] = load_feature(f)\n        \n    # get last features\n    last_features = features_df.groupby('user_id')[['timestamp', 'mean_content_accuracy', \n                                                    'last_correct_time_elapsed', 'last_incorrect_time_elapsed',\n                                                    'last_interaction_elapsed_time_l1', 'last_interaction_elapsed_time_l2', \n                                                    'last_interaction_elapsed_time_l3']].last()\n    \n    # last correct/incorrect time elapsed\n    last_correct_features = features_df.groupby(['user_id', 'answered_correctly'])['timestamp'].last()\n    \n    # drop features only used for last feature computation\n    features_df.drop([ 'timestamp', 'last_interaction_elapsed_time_l1', 'last_interaction_elapsed_time_l2', 'last_interaction_elapsed_time_l3',\n                        'last_correct_time_elapsed', 'last_incorrect_time_elapsed'], axis=1, inplace=True)\n        \n    # compute user features over all train data\n    features_df_grouped_by_user = features_df[['user_id', 'answered_correctly']].groupby('user_id')['answered_correctly']\n    mean_user_accuracy = features_df_grouped_by_user.mean().values.astype(np.float32)\n    answered_correctly_user = features_df_grouped_by_user.sum().values.astype(np.uint16)\n    answered_user = features_df_grouped_by_user.count().values.astype(np.uint16)\n    # user_mean_content_accuracy_sum for computing mean_user_content_accuracy\n    mean_content_accuracy_sum = features_df.groupby('user_id')['mean_content_accuracy'].sum().values\n    \n    del features_df_grouped_by_user, features_df\n    gc.collect()\n    \n    # get state with precomputed attempts\n    with open('/kaggle/input/riiid-answer-correctness-prediction-features/state.pkl', 'rb') as state_pickle_file:\n         state = pickle.load(state_pickle_file)\n    \n    # add all features to state\n    for idx, user_id in tqdm(enumerate(state.keys()), total=len(state)):\n        state[user_id]['mean_user_accuracy'] = mean_user_accuracy[idx]\n        state[user_id]['answered_correctly_user'] = answered_correctly_user[idx]\n        state[user_id]['answered_user'] = answered_user[idx]\n        state[user_id]['mean_content_accuracy_sum'] = mean_content_accuracy_sum[idx]\n        # last features\n        state[user_id]['timestamp'] = last_features.loc[user_id, 'timestamp']\n        state[user_id]['last_mean_content_accuracy'] = last_features.loc[user_id, 'mean_content_accuracy']\n        state[user_id]['last_correct_timestamp'] = last_correct_features.loc[user_id, True] if (user_id, True) in last_correct_features else np.nan\n        state[user_id]['last_incorrect_timestamp'] = last_correct_features.loc[user_id, False] if (user_id, False) in last_correct_features else np.nan\n        state[user_id]['last_interaction_elapsed_time_l1'] = last_features.loc[user_id, 'last_interaction_elapsed_time_l1']\n        state[user_id]['last_interaction_elapsed_time_l2'] = last_features.loc[user_id, 'last_interaction_elapsed_time_l2']\n        state[user_id]['last_interaction_elapsed_time_l3'] = last_features.loc[user_id, 'last_interaction_elapsed_time_l3']\n                \n    return state\n\nstate = get_state()\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"# Example of the state for the famous user 115\ndisplay(state[124])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"# adds all new users to the state with default values\ndef add_new_users(test_df):\n    for idx, row in test_df.iterrows():\n        # check if user exists\n        if not row['user_id'] in state:\n            state[row['user_id']] = get_new_user(row)\n\n# Gives the state for a new user with all default values\ndef get_new_user(row):\n    return {\n        'mean_user_accuracy': 0.680,\n        'answered_correctly_user': 0,\n        'answered_user': 0,\n        'user_content_attempts': dict(),\n        'timestamp': row['timestamp'],\n        'last_mean_content_accuracy': 0,\n        'last_correct_timestamp': np.nan,\n        'last_incorrect_timestamp': np.nan,\n        'last_interaction_elapsed_time_l1': 0,\n        'last_interaction_elapsed_time_l2': 0,\n        'last_interaction_elapsed_time_l3': 0,\n        'mean_content_accuracy_sum': 0,\n    }\n\n# returns a dictionary with a list for all user features\ndef get_user_data_dict():\n    return {\n        'mean_user_accuracy': [],\n        'answered_correctly_user': [],\n        'answered_user': [],\n        'last_interaction_elapsed_time_l1': [],\n        'last_interaction_elapsed_time_l2': [],\n        'last_interaction_elapsed_time_l3': [],\n        'mean_user_part_accuracy': [],\n        'last_correct_time_elapsed': [],\n        'last_incorrect_time_elapsed': [],\n    }","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The next function retrieves features for all user from the state and returns a dictionary with all features as shown above."},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"def get_user_data(state, test_df):\n    # updated data\n    user_data = get_user_data_dict()\n    \n    # mean first question part accuracies\n    part_first_question_mean_accuracy_dict = {1: 0.75, 2: 0.60, 3: 0.49, 4: 0.41, 5: 0.52, 6: 0.51, 7: 0.47}\n    cols = ['user_id', 'content_id', 'content_type_id', 'timestamp', 'part', 'mean_content_accuracy']\n    \n    for idx, (user_id, content_id, is_lecture, timestamp, part, mean_content_accuracy) in test_df[cols].iterrows():\n        # LECTURE\n        if is_lecture:\n            \n            # fill user data with dummy value\n            for key in user_data.keys():\n                user_data[key].append(0)\n            \n        # QUESTION\n        else:\n            part = int(part)\n            \n            # update last interaction elapsed time\n            state[user_id]['last_interaction_elapsed_time_l3'] = state[user_id]['last_interaction_elapsed_time_l2']\n            state[user_id]['last_interaction_elapsed_time_l2'] = state[user_id]['last_interaction_elapsed_time_l1']\n            if timestamp != state[user_id]['timestamp']:\n                state[user_id]['last_interaction_elapsed_time_l1'] = timestamp - state[user_id]['timestamp']\n                state[user_id]['timestamp'] = timestamp\n            \n            # add various features\n            cols = [\n                'mean_user_accuracy', 'answered_correctly_user', 'answered_user', \n                'last_interaction_elapsed_time_l1', 'last_interaction_elapsed_time_l2', 'last_interaction_elapsed_time_l3',\n            ]\n            for feature in cols:\n                user_data[feature].append(state[user_id][feature])\n            \n            state[user_id]['last_mean_content_accuracy'] = mean_content_accuracy\n            \n            # user part features\n            if f'part_{part}_count' in state[user_id]:\n                if state[user_id][f'part_{part}_sum'] == 0:\n                    user_data['mean_user_part_accuracy'].append(0)\n                else:\n                    user_data['mean_user_part_accuracy'].append(state[user_id][f'part_{part}_sum'] / state[user_id][f'part_{part}_count'])\n                \n            else:\n                state[user_id][f'part_{part}_sum'] = 0\n                state[user_id][f'part_{part}_count'] = 0\n                user_data['mean_user_part_accuracy'].append(part_first_question_mean_accuracy_dict[part])\n            \n            # last correct/incorrect time elapsed\n            user_data['last_correct_time_elapsed'].append(timestamp - state[user_id]['last_correct_timestamp'])\n            user_data['last_incorrect_time_elapsed'].append(timestamp - state[user_id]['last_incorrect_timestamp'])\n    \n    return user_data","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"# adds the attempt and retry feature to the test_df\ndef add_attempt_retry(test_df):\n    attempt = []\n    for user_id, content_id in test_df[['user_id', 'content_id']].itertuples(name=None, index=False):\n        if content_id in state[user_id]['user_content_attempts']:\n            state[user_id]['user_content_attempts'][content_id] += 1\n        else:\n            state[user_id]['user_content_attempts'][content_id] = 0\n\n        attempt.append(state[user_id]['user_content_attempts'][content_id])\n    \n    test_df['attempt'] = attempt\n    test_df['retry'] = test_df['attempt'] > 0","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This next function adds the mean_content_accuracy taking the prior_question_had_explanation and retry feature into account. an example of the effect of prior_question_had_explanation and retry are given below."},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"with open('/kaggle/input/riiid-answer-correctness-prediction-features/mean_content_accuracy_cases_dict.pickle', 'rb') as f:\n    mean_content_accuracy_cases_dict = pickle.load(f)\n\ndef add_mean_content_accuracy(test_df):\n    mean_content_accuracy = []\n    for key in test_df[['content_id', 'prior_question_had_explanation', 'retry']].itertuples(name=None, index=False):\n        # get mean content accuracy\n        if key in mean_content_accuracy_cases_dict:\n            mean_content_accuracy.append(mean_content_accuracy_cases_dict[key])\n        else:\n            mean_content_accuracy.append(0)\n        \n    test_df['mean_content_accuracy'] = mean_content_accuracy","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"# Example of the mean_content_accuracy for quesiton 6116, the most answered question\nfor (content_id, prior_question_had_explanation, retry), mean_content_accuracy in mean_content_accuracy_cases_dict.items():\n    if content_id == 6116:\n        print(f'content_id {content_id}, prior_question_had_explanation: {prior_question_had_explanation}, retry: {retry}, mean_content_accuracy: {mean_content_accuracy}')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"After each prediction iteration the user features are updated"},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"def update_user_data(state, prev_test_df):\n    for idx, row in prev_test_df.iterrows():\n        if not row['content_type_id']:\n            answered_correctly = row['answered_correctly']\n            user_id = row['user_id']\n            part = int(row['part'])\n            # update user features\n            state[user_id]['answered_correctly_user'] += answered_correctly\n            state[user_id]['answered_user'] += 1\n            state[user_id]['mean_user_accuracy'] = state[user_id]['answered_correctly_user'] / state[user_id]['answered_user']\n            # add user part features, initialize if no part answered yet\n            state[user_id][f'part_{part}_sum'] += answered_correctly\n            state[user_id][f'part_{part}_count'] += 1\n            # update other user features\n            state[user_id]['mean_content_accuracy_sum'] += row['mean_content_accuracy']\n            # last correct/incorrect time elapsed\n            if answered_correctly:\n                state[user_id]['last_correct_timestamp'] = row['timestamp']\n            else:\n                state[user_id]['last_incorrect_timestamp'] = row['timestamp']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Submission"},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"import riiideducation\n\nenv = riiideducation.make_env()\niter_test = env.iter_test()\nprev_test_df = None\nmodel = lgb.Booster(model_file='./model.lgb')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for idx, (test_df, _) in tqdm(enumerate(iter_test)):\n    # from 2nd iteration, update user data\n    if prev_test_df is not None:\n        prev_test_df['answered_correctly'] = eval(test_df['prior_group_answers_correct'].iloc[0])\n        update_user_data(state, prev_test_df)\n        if idx < 4:\n            display(test_df)\n            display(prev_test_df)\n            \n    # fill prior question had explenation\n    test_df['prior_question_had_explanation'] = test_df['prior_question_had_explanation'].fillna(value = False).astype(bool)\n    test_df['prior_question_elapsed_time'].fillna(23916, inplace=True)\n            \n    # merge with all features\n    test_df = features_questions_df.merge(test_df, how='right', on='content_id')\n    \n    # add new users to state\n    add_new_users(test_df)\n    \n    # add attempt, retry and mean_content_accuracy\n    add_attempt_retry(test_df)\n    add_mean_content_accuracy(test_df)\n    \n    # get user data from state and update attempt\n    user_data = get_user_data(state, test_df)\n    for feature, values in user_data.items():\n        test_df[feature] = values\n    \n    # add harmonic mean\n    test_df['hmean_user_content_accuracy'] = 2 * (\n        (test_df['mean_user_accuracy'] * test_df['mean_content_accuracy']) /\n        (test_df['mean_user_accuracy'] + test_df['mean_content_accuracy'])\n    )\n\n    test_df['answered_correctly'] = model.predict(test_df[features])\n\n    env.predict(test_df.loc[test_df['content_type_id'] == 0, ['row_id', 'answered_correctly']])\n\n    # set previour test_df\n    prev_test_df = test_df.copy()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"submission = pd.read_csv('./submission.csv')\nsubmission.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"# show the first 5 predictions\nsubmission.head()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}